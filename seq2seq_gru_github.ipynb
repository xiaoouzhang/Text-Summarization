{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from nltk.tokenize import word_tokenize\n",
    "from tensorflow.python.layers.core import Dense\n",
    "import input_utils\n",
    "import os\n",
    "from tensorflow.python.tools import inspect_checkpoint as chkp\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_d=os.getcwd()+'/model_data/'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Word Embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "FILE_TO_EMBEDDINGS = '../embeddings.5k.txt'\n",
    "emb_dict, _ = input_utils.load_embeddings(FILE_TO_EMBEDDINGS)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Transform the embeddings as numpy array, only keep regular words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "emb_dict.update((x, np.array(list(y)).astype(np.float32)) for x, y in emb_dict.items())\n",
    "large_tag_list=['<loc>','<org>','<per>','<num>','<unk>','<s>','</s>']\n",
    "for tag_key in large_tag_list:\n",
    "    emb_dict.pop(tag_key)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Look up table between words an indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "word2ind={word: idx for idx, word in enumerate(emb_dict.keys())}\n",
    "ind2word={idx: word for idx, word in enumerate(emb_dict.keys())}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Construct embedding matrix for reguar words and special tags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "emb_mat=np.zeros((len(emb_dict.keys()),57))\n",
    "for ind in ind2word.keys():\n",
    "    word=ind2word[ind]\n",
    "    emb_mat[ind,:]=emb_dict[word]\n",
    "emb_mat=emb_mat[:,:50]\n",
    "#for special tags\n",
    "emb_mat_tag=np.reshape(np.random.normal(scale=1.0,size=7*50),(7,50))\n",
    "i=0\n",
    "for tag_key in large_tag_list:\n",
    "    emb_dict[tag_key]=emb_mat_tag[i,:]\n",
    "    i+=1\n",
    "#put special tags back in the dictionary    \n",
    "word2ind={word: idx for idx, word in enumerate(emb_dict.keys())}\n",
    "ind2word={idx: word for idx, word in enumerate(emb_dict.keys())}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Embedding dimension is 50, dictionary size is 5000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4993, 50)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "emb_mat.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(7, 50)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "emb_mat_tag.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Construct indices for the text file, which is the input of the seq2seq model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "with open('../data/train_input.txt', 'r') as f:\n",
    "    train_in_data = f.read()\n",
    "with open('../data/train_output.txt', 'r') as f:\n",
    "    train_out_data = f.read()\n",
    "with open('../data/train200000_input.txt', 'r') as f:\n",
    "    train_in_data = f.read()\n",
    "with open('../data/train200000_output.txt', 'r') as f:\n",
    "    train_out_data = f.read()\n",
    "'''\n",
    "with open('../data/test_input.txt', 'r') as f:\n",
    "    test_in_data = f.read()\n",
    "with open('../data/test_output.txt', 'r') as f:\n",
    "    test_out_data = f.read()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ignore the following before the loading step, except for the first run"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get the indexed text file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def paragraph2ind(source_data,word2ind,ind2word):\n",
    "    #input: text file\n",
    "    #output: indexed text with padding, max lenghth; length of each sentence\n",
    "    #length counts END token, don't count START token\n",
    "    sentence_len=[]\n",
    "    text_ind=[]\n",
    "    #handle ' \\n'\n",
    "    articles=source_data.split('</s>')\n",
    "    \n",
    "    del articles[-1]\n",
    "    for i in range(1,len(articles)):\n",
    "        articles[i]=articles[i][2:]\n",
    "    #get index\n",
    "    for a in articles:\n",
    "        article_idx=[]\n",
    "        sentence_len.append(len(a))\n",
    "        for words in a.split():\n",
    "            article_idx.append(word2ind[words])\n",
    "        article_idx.append(word2ind['</s>'])\n",
    "        text_ind.append(article_idx)\n",
    "        \n",
    "        \n",
    "    \n",
    "    max_len=max(sentence_len)\n",
    "    min_len=min(sentence_len)\n",
    "        \n",
    "    #padding\n",
    "    padded=np.zeros((len(text_ind),max_len))\n",
    "    i=0\n",
    "    for article_id in text_ind:\n",
    "        padded[i,:]=np.array(list(article_id + [word2ind['</s>']] *(max_len-len(article_id))))\n",
    "        i+=1\n",
    "    \n",
    "    return padded,max_len,min_len, np.array(sentence_len)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "load training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(2):\n",
    "    file_ind=str(int((i+20)*10000))\n",
    "    with open('../data/train'+file_ind+'_input.txt', 'r') as f:\n",
    "        train_in_data = f.read()\n",
    "    with open('../data/train'+file_ind+'_output.txt', 'r') as f:\n",
    "        train_out_data = f.read()\n",
    "    #input in training set\n",
    "    [train_in_ind_i,max_in_len,min_in_len,train_in_len_i]=paragraph2ind(train_in_data,word2ind,ind2word)\n",
    "    #cut off\n",
    "    train_in_ind_i=train_in_ind_i[:,1:201] #no start token for input\n",
    "    train_in_ind_i[:,-1]=word2ind['</s>']\n",
    "    max_in_len=200\n",
    "    train_in_len_i[train_in_len_i>200]=200\n",
    "    \n",
    "    [train_out_ind_i,max_out_len,min_out_len,train_out_len_i]=paragraph2ind(train_out_data,word2ind,ind2word)\n",
    "    #cut off\n",
    "    train_out_ind_i=train_out_ind_i[:,0:21]\n",
    "    train_out_ind_i[:,-1]=word2ind['</s>']\n",
    "    max_out_len=20\n",
    "    train_out_len_i[train_out_len_i>20]=20\n",
    "    \n",
    "    \n",
    "    if i==0:\n",
    "        train_in_ind=train_in_ind_i\n",
    "        train_in_len=train_in_len_i\n",
    "        train_out_ind=train_out_ind_i\n",
    "        train_out_len=train_out_len_i\n",
    "        \n",
    "    else:\n",
    "        train_in_ind=np.concatenate((train_in_ind,train_in_ind_i),axis=0)\n",
    "        train_in_len=np.concatenate((train_in_len,train_in_len_i),axis=0)\n",
    "        train_out_ind=np.concatenate((train_out_ind,train_out_ind_i),axis=0)\n",
    "        train_out_len=np.concatenate((train_out_len,train_out_len_i),axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#[train_in_ind,max_in_len,min_in_len,train_in_len]=paragraph2ind(train_in_data,word2ind,ind2word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#[max_in_len,min_in_len]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cutoff input len to 200"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#train_in_ind=train_in_ind[:,1:201] #no start token for input\n",
    "#train_in_ind[:,-1]=word2ind['</s>']\n",
    "#max_in_len=200\n",
    "#train_in_len[train_in_len>200]=200"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#[train_out_ind,max_out_len,min_out_len,train_out_len]=paragraph2ind(train_out_data,word2ind,ind2word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#[max_out_len,min_out_len]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cutoff output len to 20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#train_out_ind=train_out_ind[:,0:21]\n",
    "#train_out_ind[:,-1]=word2ind['</s>']\n",
    "#max_out_len=20\n",
    "#train_out_len[train_out_len>20]=20"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "[test_in_ind,max_test_in_len,min_test_in_len,test_in_len]=paragraph2ind(test_in_data,word2ind,ind2word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_in_ind=test_in_ind[:,1:201] #no start token for input\n",
    "test_in_ind[:,-1]=word2ind['</s>']\n",
    "max_test_in_len=200\n",
    "test_in_len[test_in_len>200]=200"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "[test_out_ind,_,_,test_out_len]=paragraph2ind(test_out_data,word2ind,ind2word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_out_ind=test_out_ind[:,0:21]\n",
    "test_out_ind[:,-1]=word2ind['</s>']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save(data_d+'train_in_ind.npy',train_in_ind)\n",
    "np.save(data_d+'train_in_len.npy',train_in_len)\n",
    "np.save(data_d+'train_out_ind.npy',train_out_ind)\n",
    "np.save(data_d+'train_out_len.npy',train_out_len)\n",
    "np.save(data_d+'test_in_ind.npy',test_in_ind)\n",
    "np.save(data_d+'test_in_len.npy',test_in_len)\n",
    "np.save(data_d+'test_out_ind.npy',test_out_ind)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_in_ind=np.load(data_d+'train_in_ind.npy')\n",
    "train_in_len=np.load(data_d+'train_in_len.npy')\n",
    "train_out_ind=np.load(data_d+'train_out_ind.npy')\n",
    "train_out_len=np.load(data_d+'train_out_len.npy')\n",
    "test_in_ind=np.load(data_d+'test_in_ind.npy')\n",
    "test_in_len=np.load(data_d+'test_in_len.npy')\n",
    "test_out_ind=np.load(data_d+'test_out_ind.npy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(14000,)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_in_len.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_in_len=200\n",
    "max_out_len=20\n",
    "max_test_in_len=200"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "start_token=word2ind['<s>']\n",
    "end_token=word2ind['</s>']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_size=emb_mat.shape[0]+emb_mat_tag.shape[0]\n",
    "embedding_dim=emb_mat.shape[1]\n",
    "train_size=train_in_ind.shape[0]\n",
    "batch_size=400"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_tf_reg=tf.Variable(emb_mat.astype(np.float32),name='embedding_tf_reg',trainable=True)\n",
    "embedding_tf_tag=tf.Variable(emb_mat_tag.astype(np.float32),name='embedding_tf_tag')\n",
    "embedding_all=tf.concat([embedding_tf_reg,embedding_tf_tag],0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "#embedding_all=tf.Variable(emb_mat.astype(np.float32), name='embedding_all')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ids of input sequence. The first dimension is batch size."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "X=tf.placeholder(tf.int32,[None,max_in_len],name='X')\n",
    "X_emb=tf.nn.embedding_lookup(embedding_all, X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Record the length of each sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "seq_len=tf.placeholder(tf.int32,[None],name='seq_len')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "forward_cell=tf.contrib.rnn.GRUCell(num_units=80)\n",
    "backward_cell=tf.contrib.rnn.GRUCell(num_units=80)\n",
    "[bi_outputs_tr,encoder_states_tr]=tf.nn.bidirectional_dynamic_rnn(\n",
    "    forward_cell,backward_cell,X_emb,dtype=tf.float32,sequence_length=seq_len,time_major=False)\n",
    "encoder_outputs_tr=tf.concat(bi_outputs_tr,-1)\n",
    "#encoder_outputs_tr=tf.transpose(encoder_outputs_tr0, [1, 0, 2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Decoder helper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "decoder_out=tf.placeholder(tf.int32,[None,max_out_len],name='decoder_out')\n",
    "decoder_inp=tf.placeholder(tf.int32,[None,max_out_len],name='decoder_inp')\n",
    "decoder_emb_inp=tf.nn.embedding_lookup(embedding_all, decoder_inp,name='decoder_emb_inp')\n",
    "decoder_lengths=tf.placeholder(tf.int32,[None],name='decoder_lengths')\n",
    "train_helper = tf.contrib.seq2seq.TrainingHelper(decoder_emb_inp, decoder_lengths)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define target weights for loss function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "target_weight=np.zeros((train_size,max_out_len))\n",
    "for i in range(train_size):\n",
    "    target_weight[i,:train_out_len[i]]=1\n",
    "target_weight.astype(np.float32)\n",
    "target_weight_tf=tf.placeholder(tf.float32,[None,max_out_len],name='target_weight_batch')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Decoding network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def decoder(helper, scope, ouputs_enc,states_enc,batch_decoder,seq_len_decoder,reuse=None,max_iter=None):\n",
    "    with tf.variable_scope(scope, reuse=reuse):\n",
    "        attention_mechanism = tf.contrib.seq2seq.BahdanauAttention(\n",
    "            num_units=80, memory=ouputs_enc,\n",
    "            memory_sequence_length=seq_len_decoder)\n",
    "        decoder_cell = tf.contrib.rnn.GRUCell(num_units=80)\n",
    "        attn_cell = tf.contrib.seq2seq.AttentionWrapper(\n",
    "            decoder_cell, attention_mechanism, attention_layer_size=40,alignment_history=True)\n",
    "        init_st=attn_cell.zero_state(dtype=tf.float32,batch_size=batch_decoder).clone(cell_state=states_enc)\n",
    "        #init_st=states_enc\n",
    "        projection_layer = Dense(units=vocab_size,use_bias=False)\n",
    "        decoder = tf.contrib.seq2seq.BasicDecoder(\n",
    "            attn_cell, helper, init_st, output_layer=projection_layer)\n",
    "        outputs, states ,_= tf.contrib.seq2seq.dynamic_decode(\n",
    "            decoder,impute_finished=True,maximum_iterations=max_iter)\n",
    "        return outputs,states\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Decoder output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "#train_outputs, _ ,_= tf.contrib.seq2seq.dynamic_decode(decoder,impute_finished=True)\n",
    "train_outputs,train_states=decoder(train_helper,'decode',encoder_outputs_tr,encoder_states_tr[1],batch_size,seq_len)\n",
    "logits = train_outputs.rnn_output\n",
    "Y_train=train_outputs.sample_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "crossent = tf.nn.sparse_softmax_cross_entropy_with_logits(\n",
    "    labels=decoder_out, logits=logits)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define Training op"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "saver = tf.train.Saver()\n",
    "learning_rate=0.005\n",
    "max_gradient_norm=3\n",
    "loss = tf.reduce_sum(crossent*target_weight_tf)#target_weight\n",
    "params = tf.trainable_variables()\n",
    "gradients = tf.gradients(loss, params)\n",
    "clipped_gradients, _ = tf.clip_by_global_norm(\n",
    "    gradients, max_gradient_norm)\n",
    "optimizer = tf.train.AdamOptimizer(learning_rate=learning_rate)\n",
    "update_step = optimizer.apply_gradients(\n",
    "    zip(clipped_gradients, params))\n",
    "#training_op = optimizer.minimize(loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Inference"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define inference helper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "infer_helper = tf.contrib.seq2seq.GreedyEmbeddingHelper(embedding_all, tf.fill([100], start_token), end_token)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Testing example, which is the first sentence in the training set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test=tf.placeholder(tf.int32,[100,max_in_len],name='X_test')\n",
    "x_test=test_in_ind[:100,:]\n",
    "seq_len_test=tf.placeholder(tf.int32,[100],name='seq_len_test')\n",
    "seq_len_test_in=test_in_len[:100]\n",
    "\n",
    "x_train0=train_in_ind[:100,:]\n",
    "seq_len_train0_in=train_in_len[:100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_emb_test=tf.nn.embedding_lookup(embedding_all, X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Inference network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "#[encoder_outputs_inf,encoder_states_inf]=tf.nn.dynamic_rnn(\n",
    "#    gru_cell,X_emb_test,dtype=tf.float32,sequence_length=seq_len_test)\n",
    "[bi_outputs_inf,encoder_states_inf]=tf.nn.bidirectional_dynamic_rnn(\n",
    "    forward_cell,backward_cell,X_emb_test,dtype=tf.float32,sequence_length=seq_len_test)\n",
    "encoder_outputs_inf=tf.concat(bi_outputs_inf,-1)\n",
    "batch_test=X_test.get_shape()[0].value\n",
    "outputs_inf,states_inf=decoder(\n",
    "    infer_helper,'decode',encoder_outputs_inf, encoder_states_inf[1],batch_test,seq_len_test,reuse=True, max_iter=15)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Output id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y_test= outputs_inf.sample_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from /Users/zxo/Dropbox/Machine_Learning/course/10701/Project_10701/10701-text-summarization-project/src/model_data/model.ckpt\n"
     ]
    }
   ],
   "source": [
    "with tf.Session() as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    saver.restore(sess, data_d+\"model.ckpt\")\n",
    "    \n",
    "    #random shuffle\n",
    "    sample_size=train_in_ind.shape[0]\n",
    "    rand_index=np.random.shuffle(np.arange(sample_size))\n",
    "    train_in_ind_rand=train_in_ind[rand_index,:]\n",
    "    train_out_ind_rand=train_out_ind[rand_index,:]\n",
    "    train_in_len_rand=train_in_len[rand_index]\n",
    "    train_out_len_rand=train_out_len[rand_index]\n",
    "    target_weight_rand=target_weight[rand_index,:]\n",
    "    \n",
    "    '''\n",
    "    for i in range(31):\n",
    "        loss_value=0\n",
    "        for j in range(int(train_size/batch_size)):\n",
    "            x_in_batch=train_in_ind[j*batch_size:j*batch_size+batch_size]\n",
    "            x_out_batch=train_out_ind[j*batch_size:j*batch_size+batch_size]\n",
    "            x_in_length_batch=train_in_len[j*batch_size:j*batch_size+batch_size]\n",
    "            x_out_length_batch=train_out_len[j*batch_size:j*batch_size+batch_size]\n",
    "            target_weight_batch=target_weight[j*batch_size:j*batch_size+batch_size,:]\n",
    "            sess.run(update_step,feed_dict={X:x_in_batch, seq_len:x_in_length_batch,\\\n",
    "                                            decoder_inp:x_out_batch[:,:-1], target_weight_tf:target_weight_batch, \\\n",
    "                                            decoder_lengths:x_out_length_batch,decoder_out:x_out_batch[:,1:]})\n",
    "            loss_value+=loss.eval(feed_dict={X:x_in_batch, seq_len:x_in_length_batch,\\\n",
    "                                             decoder_inp:x_out_batch[:,:-1], target_weight_tf:target_weight_batch, \\\n",
    "                                             decoder_lengths:x_out_length_batch,decoder_out:x_out_batch[:,1:]})\n",
    "        if np.mod(i,5)==0:\n",
    "            print(i,loss_value)\n",
    "    '''\n",
    "    # get attention matrix\n",
    "    attention_images_test = sess.run(states_inf.alignment_history.stack(), feed_dict={X_test:x_test,seq_len_test:seq_len_test_in})\n",
    "    attention_images_test = np.transpose(attention_images_test, [1, 2, 0])\n",
    "    attention_images_train0 = sess.run(states_inf.alignment_history.stack(), feed_dict={X_test:x_train0,seq_len_test:seq_len_train0_in})\n",
    "    attention_images_train0 =np.transpose(attention_images_train0, [1, 2, 0])\n",
    "    #id of test samples\n",
    "    test_output_ind=(Y_test.eval(feed_dict={X_test:x_test,seq_len_test:seq_len_test_in}))\n",
    "    train0_output_ind=(Y_test.eval(feed_dict={X_test:x_train0,seq_len_test:seq_len_train0_in}))\n",
    "    save_path = saver.save(sess, data_d+\"model.ckpt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Read text from indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_output(output_ind):\n",
    "    out_text=[]\n",
    "    for i in range(output_ind.shape[0]):\n",
    "        out_article=[]\n",
    "        for j in range(output_ind.shape[1]):\n",
    "            out_article.append(ind2word[output_ind[i,j]])\n",
    "        out_text.append(out_article)\n",
    "    return out_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_out_t=read_output(train0_output_ind)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compute the f-score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "f=0.0\n",
    "for i in range(100):\n",
    "    model_out=[]\n",
    "    correct_out=[]\n",
    "    for j in range(len(train_out_t[i])):\n",
    "        if train_out_t[i][j]=='</s>':\n",
    "            break\n",
    "        model_out.append(train_out_t[i][j])\n",
    "    #print(model_out)\n",
    "    for j in range(len(train_out_correct[i])):\n",
    "        if test_out_correct[i][j]=='</s>':\n",
    "            break\n",
    "        correct_out.append(train_out_correct[i][j])\n",
    "    f_i=input_utils.fscore(model_out,correct_out)\n",
    "    #print(f_i)\n",
    "    f+=f_i\n",
    "f/=100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.39465906605339"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
